{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3wN6lUcst-i",
        "outputId": "cd2699b4-7d03-4362-e013-65ad0827b957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.4)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=b6576914b7cdf1c58f6d5dcf8dd4d8a61cc44943e84f3c5e286144797e238816\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "pip install spacy pandas seqeval tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"cweyyy/conll03\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVnSothIsvzG",
        "outputId": "7f35eba4-8df6-4e82-cd8a-2d333c9605cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'conll03' dataset.\n",
            "Path to dataset files: /kaggle/input/conll03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"cweyyy/conll03\")\n",
        "print(path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh5U_Z1jvvty",
        "outputId": "2b9a1bdc-5948-4777-fdb8-d7cd2d54b254"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'conll03' dataset.\n",
            "/kaggle/input/conll03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iphvCncUvwLl",
        "outputId": "1b65f493-88c1-4938-ef36-52f87af0bf47"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eng.dev.tsv  eng.test.tsv  eng.train.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = f\"{path}/eng.train.tsv\"\n",
        "dev_path   = f\"{path}/eng.dev.tsv\"\n",
        "test_path  = f\"{path}/eng.test.tsv\"\n"
      ],
      "metadata": {
        "id": "VRrIy2PNvz9R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Task 4: Named Entity Recognition (NER) from News Articles\n",
        "# Dataset: CoNLL-2003 (Kaggle - cweyyy/conll03, TSV format)\n",
        "# Approach: Rule-based + Model-based (spaCy)\n",
        "# ====================================================\n",
        "\n",
        "# Install dependencies\n",
        "!pip install spacy pandas tqdm seqeval kagglehub\n",
        "\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "from spacy.training import Example\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "# ====================================================\n",
        "# Step 1. Load dataset from KaggleHub\n",
        "# ====================================================\n",
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"cweyyy/conll03\")\n",
        "print(\"Dataset path:\", path)\n",
        "\n",
        "# Your files are TSV:\n",
        "train_path = Path(path) / \"eng.train.tsv\"\n",
        "dev_path   = Path(path) / \"eng.dev.tsv\"\n",
        "test_path  = Path(path) / \"eng.test.tsv\"\n",
        "\n",
        "print(\"Train file:\", train_path.exists())\n",
        "print(\"Dev file:\", dev_path.exists())\n",
        "print(\"Test file:\", test_path.exists())\n",
        "\n",
        "# ====================================================\n",
        "# Step 2. Helper functions (parser + converters)\n",
        "# ====================================================\n",
        "\n",
        "def parse_conll(filepath: Path):\n",
        "    \"\"\"Parse a CoNLL TSV file and return list of sentences (list of token columns).\"\"\"\n",
        "    sentences = []\n",
        "    with filepath.open('r', encoding='utf-8') as f:\n",
        "        sentence = []\n",
        "        for raw in f:\n",
        "            line = raw.strip()\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    sentence = []\n",
        "                continue\n",
        "            if line.startswith('-DOCSTART-'):\n",
        "                continue\n",
        "            parts = line.split(\"\\t\")   # <-- tab separated\n",
        "            if parts:\n",
        "                sentence.append(parts)\n",
        "        if sentence:\n",
        "            sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "def tokens_to_text_and_offsets(tokens):\n",
        "    \"\"\"Join tokens into text and compute char offsets for each token.\"\"\"\n",
        "    text = ' '.join(tokens)\n",
        "    offsets = []\n",
        "    pos = 0\n",
        "    for tok in tokens:\n",
        "        start = pos\n",
        "        end = start + len(tok)\n",
        "        offsets.append((start, end))\n",
        "        pos = end + 1\n",
        "    return text, offsets\n",
        "\n",
        "def bio_to_token_entities(tags):\n",
        "    \"\"\"Convert BIO tags to (start,end,label) entity spans (token indices).\"\"\"\n",
        "    entities = []\n",
        "    ent_label = None\n",
        "    ent_start = None\n",
        "    ent_end = None\n",
        "    for i, tag in enumerate(tags):\n",
        "        if tag == 'O':\n",
        "            if ent_label is not None:\n",
        "                entities.append((ent_start, ent_end, ent_label))\n",
        "                ent_label = None\n",
        "            continue\n",
        "        if tag.startswith('B-'):\n",
        "            if ent_label is not None:\n",
        "                entities.append((ent_start, ent_end, ent_label))\n",
        "            ent_label = tag[2:]\n",
        "            ent_start = i\n",
        "            ent_end = i + 1\n",
        "        elif tag.startswith('I-') and ent_label is not None and tag[2:] == ent_label:\n",
        "            ent_end = i + 1\n",
        "        else:\n",
        "            if ent_label is not None:\n",
        "                entities.append((ent_start, ent_end, ent_label))\n",
        "            ent_label = None\n",
        "    if ent_label is not None:\n",
        "        entities.append((ent_start, ent_end, ent_label))\n",
        "    return entities\n",
        "\n",
        "def token_entities_to_char(entities_tokenidx, tokens):\n",
        "    \"\"\"Convert token index entities into character span entities.\"\"\"\n",
        "    text, offsets = tokens_to_text_and_offsets(tokens)\n",
        "    char_entities = []\n",
        "    for s_idx, e_idx, lbl in entities_tokenidx:\n",
        "        char_start = offsets[s_idx][0]\n",
        "        char_end = offsets[e_idx - 1][1]\n",
        "        char_entities.append((char_start, char_end, lbl))\n",
        "    return text, char_entities\n",
        "\n",
        "def conll_to_spacy_data(filepath: Path):\n",
        "    \"\"\"Convert a CoNLL TSV file to spaCy training data and parsed sentences.\"\"\"\n",
        "    parsed = parse_conll(filepath)\n",
        "    data = []\n",
        "    parsed_sentences = []\n",
        "    for sent in tqdm(parsed, desc=f\"Parsing {filepath.name}\"):\n",
        "        tokens = [cols[0] for cols in sent]\n",
        "        tags = [cols[-1] for cols in sent]  # NER tags are last column\n",
        "        token_entities = bio_to_token_entities(tags)\n",
        "        text, char_entities = token_entities_to_char(token_entities, tokens)\n",
        "        ann = {\"entities\": [(s, e, l) for s, e, l in char_entities]}\n",
        "        data.append((text, ann))\n",
        "        parsed_sentences.append((tokens, tags))\n",
        "    return data, parsed_sentences\n",
        "\n",
        "# ====================================================\n",
        "# Step 3. Convert dataset\n",
        "# ====================================================\n",
        "\n",
        "train_data, train_parsed = conll_to_spacy_data(train_path)\n",
        "dev_data, dev_parsed     = conll_to_spacy_data(dev_path)\n",
        "test_data, test_parsed   = conll_to_spacy_data(test_path)\n",
        "\n",
        "print(\"Train sentences:\", len(train_data))\n",
        "print(\"Dev sentences:\", len(dev_data))\n",
        "print(\"Test sentences:\", len(test_data))\n",
        "\n",
        "# Quick EDA\n",
        "counts = Counter([ent[2] for _, ann in train_data for ent in ann['entities']])\n",
        "print(\"Entity counts:\", counts)\n",
        "\n",
        "# ====================================================\n",
        "# Step 4. Rule-based baseline\n",
        "# ====================================================\n",
        "import re\n",
        "from spacy.language import Language\n",
        "from spacy.pipeline import EntityRuler\n",
        "\n",
        "nlp_rule = spacy.blank('en')\n",
        "ruler = nlp_rule.add_pipe('entity_ruler')\n",
        "ruler.add_patterns([\n",
        "    {\"label\": \"ORG\", \"pattern\": \"United Nations\"},\n",
        "    {\"label\": \"MISC\", \"pattern\": \"COVID-19\"},\n",
        "])\n",
        "\n",
        "@Language.component('regex_person')\n",
        "def regex_person(doc):\n",
        "    new_ents = list(doc.ents)\n",
        "    for m in re.finditer(r\"\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){0,2})\\b\", doc.text):\n",
        "        span = doc.char_span(m.start(), m.end(), label='PER')\n",
        "        if span is not None:\n",
        "            new_ents.append(span)\n",
        "    doc.ents = new_ents\n",
        "    return doc\n",
        "\n",
        "nlp_rule.add_pipe('regex_person', last=True)\n",
        "\n",
        "print(\"\\nRule-based sample:\")\n",
        "for t, _ in test_data[:3]:\n",
        "    doc = nlp_rule(t)\n",
        "    print(t[:80], \"...\")\n",
        "    print([(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "# ====================================================\n",
        "# Step 5. Train spaCy model\n",
        "# ====================================================\n",
        "\n",
        "def build_nlp(base_model=None):\n",
        "    if base_model:\n",
        "        print(\"Loading base model\", base_model)\n",
        "        nlp = spacy.load(base_model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")\n",
        "    return nlp\n",
        "\n",
        "def train_spacy_ner(nlp, train_data, n_iter=3, drop=0.3, dev_data=None):\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.add_pipe('ner')\n",
        "    else:\n",
        "        ner = nlp.get_pipe('ner')\n",
        "\n",
        "    labels = set()\n",
        "    for _, ann in train_data:\n",
        "        for _, _, l in ann[\"entities\"]:\n",
        "            labels.add(l)\n",
        "    for lab in labels:\n",
        "        ner.add_label(lab)\n",
        "\n",
        "    # Initialize with small examples\n",
        "    init_examples = []\n",
        "    for text, ann in train_data[:100]:\n",
        "        doc = nlp.make_doc(text)\n",
        "        init_examples.append(Example.from_dict(doc, ann))\n",
        "    nlp.initialize(lambda: init_examples)\n",
        "\n",
        "    for itn in range(n_iter):\n",
        "        random.shuffle(train_data)\n",
        "        losses = {}\n",
        "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
        "        for batch in batches:\n",
        "            examples = []\n",
        "            for text, ann in batch:\n",
        "                doc = nlp.make_doc(text)\n",
        "                examples.append(Example.from_dict(doc, ann))\n",
        "            nlp.update(examples, drop=drop, losses=losses)\n",
        "        print(f\"Epoch {itn+1}/{n_iter} - Losses: {losses}\")\n",
        "    return nlp\n",
        "\n",
        "# Train small model (keep epochs low on Colab CPU)\n",
        "nlp_model = build_nlp()\n",
        "nlp_model = train_spacy_ner(nlp_model, train_data[:2000], n_iter=3)\n",
        "\n",
        "# ====================================================\n",
        "# Step 6. Evaluate with seqeval\n",
        "# ====================================================\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "def predict_bio_for_tokens(nlp, tokens):\n",
        "    text, offsets = tokens_to_text_and_offsets(tokens)\n",
        "    doc = nlp(text)\n",
        "    pred_spans = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
        "    pred_bio = []\n",
        "    for (t_start, t_end) in offsets:\n",
        "        label = 'O'\n",
        "        for ps, pe, pl in pred_spans:\n",
        "            if t_start >= ps and t_end <= pe:\n",
        "                label = 'B-' + pl if t_start == ps else 'I-' + pl\n",
        "                break\n",
        "        pred_bio.append(label)\n",
        "    return pred_bio\n",
        "\n",
        "def evaluate_with_seqeval(nlp, parsed_sentences):\n",
        "    y_true, y_pred = [], []\n",
        "    for tokens, gold_tags in tqdm(parsed_sentences, desc=\"Evaluating\"):\n",
        "        y_true.append(gold_tags)\n",
        "        y_pred.append(predict_bio_for_tokens(nlp, tokens))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "evaluate_with_seqeval(nlp_model, dev_parsed[:500])\n",
        "\n",
        "# ====================================================\n",
        "# Step 7. Visualize entities\n",
        "# ====================================================\n",
        "from spacy import displacy\n",
        "\n",
        "print(\"\\nVisualization sample:\")\n",
        "doc = nlp_model(\"Barack Obama visited Germany and met Angela Merkel at the United Nations.\")\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)\n",
        "\n",
        "# ====================================================\n",
        "# Step 8. Save model to Google Drive\n",
        "# ====================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/spacy_conll03_model\"\n",
        "nlp_model.to_disk(output_dir)\n",
        "print(\"Model saved to:\", output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4qSgppbKv_8q",
        "outputId": "e281a985-ad13-48e6-e522-8464c77d6625"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.4)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Using Colab cache for faster access to the 'conll03' dataset.\n",
            "Dataset path: /kaggle/input/conll03\n",
            "Train file: True\n",
            "Dev file: True\n",
            "Test file: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing eng.train.tsv: 100%|██████████| 14041/14041 [00:00<00:00, 156469.29it/s]\n",
            "Parsing eng.dev.tsv: 100%|██████████| 3250/3250 [00:00<00:00, 140844.44it/s]\n",
            "Parsing eng.test.tsv: 100%|██████████| 3453/3453 [00:00<00:00, 15363.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train sentences: 14041\n",
            "Dev sentences: 3250\n",
            "Test sentences: 3453\n",
            "Entity counts: Counter({'PER': 4284, 'ORG': 2485, 'LOC': 1041, 'MISC': 858})\n",
            "\n",
            "Rule-based sample:\n",
            "SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT . ...\n",
            "[]\n",
            "Nadim Ladki ...\n",
            "[('Nadim Ladki', 'PER')]\n",
            "AL-AIN , United Arab Emirates 1996-12-06 ...\n",
            "[('United Arab Emirates', 'PER')]\n",
            "Epoch 1/3 - Losses: {'ner': np.float32(1968.1443)}\n",
            "Epoch 2/3 - Losses: {'ner': np.float32(900.50055)}\n",
            "Epoch 3/3 - Losses: {'ner': np.float32(743.0066)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 500/500 [00:01<00:00, 387.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.00      0.00      0.00       268\n",
            "        MISC       0.00      0.00      0.00        85\n",
            "         ORG       0.00      0.00      0.00       195\n",
            "         PER       0.00      0.00      0.00       332\n",
            "\n",
            "   micro avg       0.00      0.00      0.00       880\n",
            "   macro avg       0.00      0.00      0.00       880\n",
            "weighted avg       0.00      0.00      0.00       880\n",
            "\n",
            "\n",
            "Visualization sample:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Barack\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " Obama visited Germany and met \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Angela\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " Merkel at the \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    United\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " Nations.</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model saved to: /content/drive/MyDrive/spacy_conll03_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OF3BYttNwjfd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}